groups:
# 1.	Monitor the k8s cluster
# 1.1.	Node is down/not ready:
- name: AWS-K8S-JobDown.blackbox.rules
  rules:
  - alert: AWS-K8S-JobDown
    expr: up == 0
    for: 5m
    labels:
      service: kubernetes-monitoring-blackbox
      severity: warning
    annotations:
        description: "Instance {{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."
        summary: "Instance {{ $labels.instance }} down"

# - name: K8S-NodeNotReady.blackbox.rules
#   rules:
#   - alert: K8S-NodeNotReady
#     expr: kube_node_status_condition{condition="Ready",status="false"} == 1
#     for: 5m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Node {{ $labels.node }} in state not ready for more than 5 minutes."
#       summary: "{{ $labels.node }} : not ready"

# # 1.2.	Memory usage critical/pressure
# - name: K8S-MemoryPressure.blackbox.rules
#   rules:
#   - alert: K8S-MemoryPressure
#     expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
#     for: 5m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Node {{ $labels.node }} in memory pressure for more than 5 minutes."
#       summary: "{{ $labels.node }} : memory pressure"


# # 1.3.	Disk usage critical/pressure
# - name: K8S-DiskPressure.blackbox.rules
#   rules:
#   - alert: K8S-DiskPressure
#     expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
#     for: 5m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Node {{ $labels.node }} in disk pressure for more than 5 minutes."
#       summary: "{{ $labels.node }} : disk pressure"

# - name: K8S-OutOfDisk.blackbox.rules
#   rules:
#   - alert: K8S-OutOfDisk
#     expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
#     for: 5m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Node {{ $labels.node }} in out of disk for more than 5 minutes."
#       summary: "{{ $labels.node }} : out of disk"


# # 2.	Monitor the pods
# # 2.1.	CPU usage critical
# - name: K8S-CPUUsageCritical.blackbox.rules
#   rules:
#   - alert: K8S-CPUUsageCritical
#     expr: container_cpu_usage_seconds_total{pod_name!="", name=~"k8s_POD_.*"} > 0.99 
#     for: 30m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "CPU usage exceeds 99% for {{ $labels.namespace }}/{{ $labels.pod_name }}"
#       summary: "CPU usage exceeds 99%  for {{ $labels.namespace }}/{{ $labels.pod_name }}" 


# # 2.2.	Memory usage critical
# - name: K8S-MemoryUsageCritical.blackbox.rules
#   rules:
#   - alert: K8S-MemoryUsageCritical
#     expr: container_memory_usage_bytes{pod_name != ""} > 0.99 * container_spec_memory_limit_bytes{pod_name != ""} and container_spec_memory_limit_bytes{pod_name != ""} != 0
#     for: 30m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Memory usage exceeds 99%  for {{ $labels.namespace }}/{{ $labels.pod_name }}"
#       summary: "Memory usage exceeds 99%  for {{ $labels.namespace }}/{{ $labels.pod_name }}" 


# # 2.3.	Disk usage critical
# - name: K8S-DiskUsageCritical.blackbox.rules
#   rules:
#   - alert: K8S-DiskUsageCritical
#     expr: container_fs_usage_bytes{pod_name!=""} > 0.9 * container_fs_limit_bytes{pod_name!=""} 
#     for: 30m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Disk usage exceeds 90%  for {{ $labels.namespace }}/{{ $labels.pod_name }}"
#       summary: "Disk usage exceeds 90%  for {{ $labels.namespace }}/{{ $labels.pod_name }}" 


# # 2.4.	Network errors
# - name: K8S-NetworkRXError.blackbox.rules
#   rules:
#   - alert: K8S-NetworkRXError
#     expr: rate (container_network_receive_errors_total{pod_name=~"^.+$"}[1m]) > 0
#     for: 15m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Network receive errors {{ $labels.namespace }}/{{ $labels.pod_name }} for last 15 min"
#       summary: "Network receive errors:{{ $labels.namespace }}/{{ $labels.pod_name }}" 

# - name: K8S-NetworkTXError.blackbox.rules
#   rules:
#   - alert: K8S-NetworkTXError
#     expr: rate (container_network_transmit_errors_total{pod_name=~"^.+$"}[1m]) > 0
#     for: 15m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Network transmit errors {{ $labels.namespace }}/{{ $labels.pod_name }} for last 15 min"
#       summary: "Network transmit errors:{{ $labels.namespace }}/{{ $labels.pod_name }}" 


# # 2.5.	Not enough available pods compared to the desired pods
# - name: K8S-PodsNumber.blackbox.rules
#   rules:
#   - alert: K8S-PodsNumber
#     expr: kube_replicaset_status_replicas < kube_replicaset_spec_replicas
#     for: 10m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Not enough pods running for application {{ $labels.namespace }}/{{ $labels.deployment }}"
#       summary: "Not enough pods running"

# # 2.6.	Pod restart too much
# - name: K8S-PodRestartingTooMuch.blackbox.rules
#   rules:
#   - alert: K8S-PodRestartingTooMuch
#     expr: rate(kube_pod_container_status_restarts_total[5m]) > 0
#     for: 20m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Pod {{ $labels.namespace }}/{{ $labels.pod_name }} restarting too much."
#       summary: "Pod {{ $labels.namespace }}/{{ $labels.pod_name }} restarting too much."


# # 2.7.	Deployment failed
# - name: K8S-DeploymentGenerationMismatch.blackbox.rules
#   rules:
#   - alert: K8S-DeploymentGenerationMismatch
#     expr: kube_deployment_status_observed_generation{job="kubernetes-service-endpoints"} != kube_deployment_metadata_generation{job="kubernetes-service-endpoints"}
#     for: 5m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Deployment of {{ $labels.namespace }}/{{ $labels. deployment }} failed - observed replicas != intended replicas."
#       summary: "Deployment of {{ $labels.namespace }}/{{ $labels. deployment }} failed"

# # 2.8.	No available Replicaset for deployment
# - name: K8S-DeploymentReplicasUnavailable.blackbox.rules
#   rules:
#   - alert: K8S-DeploymentReplicasUnavailable
#     expr: kube_deployment_status_replicas_unavailable > 0
#     for: 1m
#     labels:
#       service: kubernetes-monitoring-blackbox
#       severity: warning
#     annotations:
#       description: "Some replicas of deployment {{ $labels.deployment }} are unavailable "
#       summary: "{{ $labels.deployment }} : replicas unavailable"

- name: AWS-Pod-Failing.rules
  rules:                                                                                                                        
  - alert: AWS-PodFailing
    expr: rate(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", label_app != "job"}[1h]) != 0 or rate(kube_pod_container_status_waiting_reason{reason="ImagePullBackOff", label_app != "job"}[1h]) != 0
    for: 20s
    labels:
      service: kubernetes-monitoring-blackbox
    annotations:
      description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing.
      summary: => {{ $labels.namespace }}/{{ $labels.pod }} failing.

- name: AWS-PodCreationFailed.rules
  rules:
  - alert: AWS-PodCreationFailed
    expr: kube_pod_container_status_waiting_reason{reason="ContainerCreating"} != 0
    for: 30m
    labels:
      service: kubernetes-monitoring-blackbox
    annotations:
      description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing to create for the last 30 minutes.
      summary: => {{ $labels.namespace }}/{{ $labels.pod }} failing.



